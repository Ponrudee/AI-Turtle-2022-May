สวัสดีครับ วันนี้พวกเรากลุ่มการสร้างงานศิลปะมัลติมีเดียเชิงพาณิชย์โดยการฝึกสอนโครงข่ายประสาทเทียม 
โดยมีอาจารย์ที่ปรึกษาคือ รองศาสตราจารย์.ดร. พรฤดี เนติโสภากุล
กระผมนายภาสกร นุชิตขจรวุฒิ รหัสนักศึกษา61070164ครับ

==============================================================

ต่อไปคือกระบวนการดำเนินงานครับ โดยจะกล่าวถึงกระบวนการเดิมที่ได้ทำไว้ 
ต่อด้วยภาพรวมกระบวนการใหม่ที่ได้ทำ ซึ่งจะประกอบไปด้วยการสร้างรูปภาพ การสร้างเพลง การสร้างภาพเคลื่อนไหว และปิดท้ายด้วยการวางขายผลงานในรูปแบบ NFT ครับ

เริ่มที่กระบวนการเดิม พวกเราได้ใช้ Deep Convolutional Generative Adversarial Network หรือที่เรียกสั้น ๆ ว่า DCGAN มาใช้ในการสร้างรูปภาพ โดยที่โครงข่ายจะพยายามสร้างรูปภาพเลียนแบบให้เหมือนภาพจริงที่โครงข่ายได้ฝึก ข้อเสียคือไม่สามารถกำหนดอินพุตเพื่อควบคุมผลลัพธ์รูปภาพที่โครงข่ายสร้างได้ 
ซึ่งเราได้นำ DCGAN จากสามที่มาทดสอบคือ จากTensorFlow จากคุณAnder Fernadez และจาก PyTorch ซึ่งได้ผลการทดสอบคือ DCGAN จาก TensorFlow ทำงานได้รวดเร็วที่สุด เราจึงเลือกพัฒนาการสร้างรูปภาพโดยใช้ไลบรารี TensorFlow ครับ
กระบวนการเดิมอีกส่วนหนึ่งคือภาพเคลื่อนไหวครับ ซึ่งเราได้ใช้โครงข่าย Convolution LSTM โดยฝึกสอนโดยใช้ชุดข้อมูลรูปสัตว์สี่ขาเคลื่อนที่ ซึ่งเรานำมาทดลองกับรูปภาพที่ได้จาก DCGAN ก็ได้ผลลัพธ์ที่ไม่ชัดเจนดังภาพทางขวาครับ

หัวข้อต่อไปคือภาพรวมการดำเนินงานปัจจุบันครับ ซึ่งจะเป็นการสร้างรูปภาพโดยใช้ Image-to-Image Translation with Conditional Adversarial Networks หรือเรียกย่อว่า pix
pix cGAN เพื่อสร้างรูปภาพและนำมาเรียงเป็นวิดีโอเฟรมเรทต่ำ จากนั้นนำเข้าสู่แอปพลิเคชัน Real-Time Intermediate Flow Estimation for Video Frame Interpolation หรือเรียกย่อว่า RIFE มาเพิ่มเฟรมเรท ทางด้านการสร้างดนตรีเราก็ใช้โครงข่าย Recurrent Neural Networks มาสร้างเพลง และนำวิดีโอมารวมกับเพลงเพื่อนำไปขายในรูปแบบ NFT

ต่อมาเราจะเจาะเข้าไปในแต่ละกระบวนการ เริ่มโดยกระบวนการสร้างรูปภาพ เราใช้ pixpix cGAN โดยสามารถสร้างผลลัพธ์ได้โดยมีการกำหนดอินพุตเพื่อควบคุมการปรับโครงสร้างรูปภาพ โดย pix2pix cGAN จะเรียนรู้ข้อมูลอินพุตควบคู่ไปกับรูปภาพจริงและรูปภาพผลลัพธ์ โดยโครงข่าย generator จะสร้างรูปภาพผลลัพธ์จากข้อมูลอินพุต และนำรูปภาพผลลัพธ์ควบคู่ไปกับข้อมูลอินพุตไปให้โครงข่าย discriminator เรียนรู้ภาพที่ถูกโครงข่าย generator สร้างขึ้น รวมถึงให้ discriminator เรียนรู้ภาพข้อมูลจริงควบคู่ไปกับข้อมูลอินพุตเพื่อเรียนรู้การคัดแยกภาพจริงและภาพที่ generator สร้าง 
โดยเบื้องต้น เราได้นำชุดข้อมูลรูปสัตว์มาสร้างอินพุตสามแบบ คือ 1. อินพุตแบบลายเส้นของสัตว์ 2. อินพุตเฉพาะลายเส้นขอบนอก และ 3.อินพุตแบบใช้สีในการแบ่งส่วนร่างกาย โดยผลลัพธ์จะเห็นว่าอินพุตที่มีรายละเอียดมากอย่างภาพเส้นทางซ้ายสุดจะสร้างรูปที่มีรายละเอียดได้ดีกว่า แต่การสร้างรูปภาพอาจไปในทิศทางเดียวกันของชุดข้อมูลส่วนใหญ่ที่ไม่สามารถควบคุมได้ ขาดความหลากหลาย เช่น ภาพมีโทนสีเดิม คือโทนสีน้ำตาล
ต่อมาเราจึงลดความซับซ้อนลงโดยการกำหนดรูปภาพให้เป็นในทิศทางเดียวกัน คือวาดรูปภาพเต่าแบบลายเส้นขึ้นมาเอง พร้อมทั้งวาดเครื่องประดับเช่นแว่นกับหมวก แล้วนำชุดข้อมูลพื้นผิว texture มาทำกระบวนการ image processing เพื่อสร้างชุดข้อมูลเต่าขึ้นมาเองกว่า 6000 รูป 
โดยดังตัวอย่างรูปที่นำมาฝึก จะเป็นภาพคู่ของภาพจริง และภาพอินพุต โดยภาพอินพุตจะเป็นการล่นระยะขอบของสีบนตัวและลวดลายบนกระดอง โดยใช้กระบวนการกร่อน หรือที่เรียกว่า erode ขนาด mask ในการลงสี
หลังจากฝึกโครงข่ายแล้ว เราได้ทดสอบโดยการลงสีเต่า 4 แบบและได้ผลลัพธ์ตามภาพในสไลด์ โดยภาพซ้ายบน ผลลัพธ์ไม่ค่อยดีอาจเป็นเพราะใช้สีขาวและดำซึ่งเป็นสีของพื้นที่ว่างและลายเส้น ในภาพขวาบนและซ้ายล่างมีการลงสีลายกระดองโดยใช้สองสี และวาดแว่นกับหมวกใหม่โดยไม่ซ้ำกับแว่นและหมวกในชุดข้อมูล ผลลัพธ์ก็ยังสามารถลงสีได้ดี และในภาพสุดท้ายมุมขวาล่าง ก็ใช้หมวกเดียวกับที่มีในชุดข้อมูล และลงสีบนกระดองแบบหลากหลายสี  ผลลัพธ์ก็ยังสามารถลงสีได้ดีเช่นกัน

ต่อมาการสร้างเพลง เราได้ใช้โครงข่าย Recurrent Neural Network โดยเป็นโครงข่ายที่นำผลลัพธ์ในลำดับก่อนหน้ามาใช้ในการคาดการณ์ผลลัพธ์ตัวต่อไป ซึ่งเหมาะสมกับเพลงที่มีการเรียงลำดับโน้ตดนตรี  ซึ่งเราได้นำโครงข่ายมาจาก MIT Deeplearning ซึ่งได้ทำโครงข่ายประสาทเทียมสำหรับสร้างโน้ตเพลงไว้ โดยใช้ชุดข้อมูลในรูปแบบ abc notation
ซึ่ง abc notation คือข้อมูลรูปแบบ text ที่ให้คอมพิวเตอร์อ่านเพื่อสร้างเสียงเพลงขึ้นมา โดยแบ่งเป็นส่วน header คือบรรทัดที่ขึ้นต้นด้วย X: จนถึง K: และบรรทัดต่อจากนั้นคือการเรียงของโน้ตดนตรี
แล้วเราได้ทำการ pre process ชุดข้อมูลโดยการตัดบรรทัดที่ไม่จำเป็นต่อการเปล่งเสียงออก เพื่อให้การฝึกโครงข่ายมีประสิทธิภาพมากขึ้น
โดยเราได้ทดสอบโดยฝึกโครงข่ายเป็นจำนวน 1 แสนรอบและให้สร้างตัวอักษรยาว 1 หมื่นตัวอักษร ซึ่งเราใช้ชุดข้อมูลเพลงของนักดนตรีชื่อดังอย่าง Beethoven, Mozart และ Bach มาใช้ในการฝึก โดยชุดข้อมูลของ Beethoven คนเดียว หรือชุดข้อมูลที่รวมของทั้งสามคนเข้าด้วยกัน ก็มีผลลัพธ์จำนวน 0-2 เพลงในการทดสอบ generate ตัวอักษรขึ้นมาในแต่ละครั้ง จากนั้นเราจึงเปลี่ยนแนวคิดของชุดข้อมูลโดยแบ่งเป็นเพลงประเภทเร็วและเพลงประเภทช้าออกจากกัน ซึ่งได้ผลลัพธ์จำนวนเพลงมากขึ้นจากการ generate ตัวอักษรขึ้นมาในแต่ละครั้ง ดังนั้นการเลือกชุดข้อมูลที่มีลักษณะใกล้เคียงกันจะมีแนวโน้มในการฝึกโครงข่ายประสาทเทียมให้สามารถสร้างผลลัพธ์ได้จำนวนมากกว่า 
และสไลด์ต่อไปนี้คือตัวอย่างผลลัพธ์เพลงที่ได้ครับ

ต่อมาคือการสร้างภาพเคลื่อนไหว ซึ่งเราได้ใช้แอป Real-Time Intermediate Flow Estimation for Video Frame Interpolation หรือเรียกย่อว่า RIFE ซึ่งเป็นแอปที่มีพื้นฐานมาจากโครงข่าย IFNet เพื่อมาใช้ในการสร้างเฟรมระหว่างเฟรมดั้งเดิมของวิดีโอที่มีอยู่ หรือที่เรียกว่าการ Interpolation 
โดยเราได้ทดสอบสี่แบบ คือ 
1.มีการเคลื่อนที่ของเครื่องประดับและลวดลายเพียงเล็กน้อย
2.มีการเคลื่อนที่ของเครื่องประดับและลวดลายอย่างมาก
3.มีการเปลี่ยนแปลงของโทนสีไปในทิศทางเดียวกันเพียงเล็กน้อย
4.มีการเปลี่ยนแปลงของโทนสีไปในทิศทางเดียวกันอย่างมาก
โดยแบบแรกที่มีการขยับเล็กน้อยของหมวกและจุดสีฟ้า จะเห็นว่าการเคลื่อนที่มีความลื่นไหล
ในขณะที่แบบที่สอง มีการขยับที่ต่างกันในแต่ละเฟรมมากขึ้น จะเห็นว่ามีช่วงเฟรมที่จุดสีฟ้าไม่ได้เคลื่อนที่แต่เป็นการหายไปแล้วสร้างขึ้นมาใหม่เลย หรือหมวกที่ขยับจนเส้นขอบศีรษะเบี้ยว
แบบที่ 3 ทดสอบสีบนกระดองที่เปลี่ยนไปในแต่ละเฟรมโดยเป็นสีที่ไม่ต่างกันมาก ก็ทำให้กระดองเปลี่ยนสีอย่างเนียนและทั่วถึง
ในขณะที่แบบที่ 4 สีบนกระดองระหว่างเฟรมจะแตกต่างกันมากขึ้น ทำให้การเปลี่ยนสีมีความกระจายออกไปไม่สม่ำเสมอ
ดังนั้นหากรายละเอียดบนรูปภาพมีการเปลี่ยนแปลงระหว่างเฟรมมาก ก็จะยิ่งส่งผลให้การสร้างเฟรมขึ้นมาใหม่สัมพันธ์กับการเปลี่ยนแปลงได้น้อยลง

จากนั้นเรานำผลลัพธ์เพลงและภาพเคลื่อนไหวที่เพิ่มเฟรมเรทแล้วมารวมกันและนำไปขายเป็นNFT
ซึ่งเราได้เลือกตลาด Opensea ที่ทำงานบน Polygon chain เพราะไม่เสียค่าธรรมเนียมแรกเข้า แต่ต้องแลกกับการที่ตลาดไม่ได้นิยมเท่า Opensea ที่ทำงานบน Ethereum chain ซึ่งเราได้ขายผลงานไป 5 รูปในคอลเลกชัน Ai turtle with music ซึ่งวางขายวันที่ 16 เมษายน 2565 และตรวจสอบผลตอบรับหลังผ่านไปได้หนึ่งสัปดาห์ มีเต่าหมายเลข 2 และหมายเลข 3 ที่มีการกดชื่นชอบ ซึ่งสไลด์ต่อเป็นจะเป็นตัวอย่างผลงานNFTที่เราได้วางขายไปครับ

สรุปผลโครงงานคือ
ในการสร้างรูปภาพ สามารถกำหนดสี ลวดลาย และเครื่องประดับ บนภาพเต่าที่เป็นพื้นฐานได้
ด้านเพลง เลียนแบบเพลงที่มนุษย์สร้างได้ปานกลาง 
ด้านภาพเคลื่อนไหว มีความต่อเนื่องหากความแตกต่างระหว่างเฟรมไม่ได้มากจนเกินไป
และสุดท้าย ผลตอบรับหลังวางขายก็มีไม่มาก
แนวทางในการพัฒนาโครงการของเราคือ การออกแบบอินพุตในการสร้างรูปภาพโดย pix2pix cGAN ให้ซับซ้อนและแปลกใหม่ขึ้น อีกทั้งหาโครงข่ายประสาทเทียมที่รองรับรูปภาพขนาดใหญ่ขึ้น
ต่อไปจะเป็นการเดโม่การสร้างผลงานของพวกเราครับ

========================================================

ต่อไปคือการสร้างเพลงครับ ซึ่งผลลัพธ์ที่ได้จาก RNN จะเป็น text ของเพลงในรูปแบบ abc notation หลายเพลง โดยเราจะทำการนำแต่ละเพลงไปเปิดในโปรแกรมที่อ่าน abc notation ได้ โดยเราใช้โปรแกรม EasyABC 
เมื่อเราเอา abc notation ใส่เข้าไป โปรแกรมก็จะอ่านออกมาเป็นโน้ตเพลงและสามารถกดฟังเพลงได้ จากนั้นเราก็จะทำการบันทึกเพลงเพื่อนำไปใช้รวมกับวิดีโอครับ